{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 445,
     "status": "ok",
     "timestamp": 1630946940277,
     "user": {
      "displayName": "Hyunji Moon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiYrsHKC2ko8bXqSu_1fLdisKmzowPXNEyNDH5ejzI=s64",
      "userId": "17944317414578830628"
     },
     "user_tz": 240
    },
    "id": "mM4VZco0rXi8",
    "outputId": "6da70f76-bfd6-4472-f5bc-4131b51ca832"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "This version of jaxlib was built using AVX instructions, which your CPU and/or operating system do not support. You may be able work around this issue by building jaxlib from source.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e0388cb606f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcmdstanpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscipy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjsc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/sbc/lib/python3.8/site-packages/jax/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# We want the exported object to be the class, so we first import the module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# to make sure a later import doesn't overwrite the class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_config_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0m_config_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/sbc/lib/python3.8/site-packages/jax/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# flake8: noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/sbc/lib/python3.8/site-packages/jax/_src/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjax_jit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/sbc/lib/python3.8/site-packages/jax/_src/lib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjaxlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcpu_feature_guard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mcpu_feature_guard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_cpu_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjaxlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxla_client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: This version of jaxlib was built using AVX instructions, which your CPU and/or operating system do not support. You may be able work around this issue by building jaxlib from source."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import cmdstanpy\n",
    "import jax\n",
    "from jax.experimental import optimizers\n",
    "import jax.scipy as jsc\n",
    "import jax.numpy as jnp\n",
    "import urllib\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "# required install (the rest is preinstalled)\n",
    "!pip install cmdstanpy==0.9.77\n",
    "#fast cmdstanpy install\n",
    "# #(faster than compiling from source via install_cmdstan() function)\n",
    "# tgz_file = 'colab-cmdstan-2.23.0.tar.gz'\n",
    "# tgz_url = 'https://github.com/stan-dev/cmdstan/releases/download/v2.23.0/colab-cmdstan-2.23.0.tar.gz'\n",
    "# if not os.path.exists(tgz_file):\n",
    "#     urllib.request.urlretrieve(tgz_url, tgz_file)\n",
    "#     shutil.unpack_archive(tgz_file)\n",
    "#import pybrms\n",
    "from matplotlib import pyplot as plt\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/robust_optimization/src')\n",
    "#######\n",
    "from generator import sim_y_barThetax, sim_ThetaXy\n",
    "#from contextPolicy import argmin_lopt_genargmin_lopt_emp, argmin_lpred, argmin_lopt_bar_argmin_lpred\n",
    "from loss import lopt_NV\n",
    "#from plot import train_test_err\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 100000\n",
    "# 1. plot for train, test loss for increasing trainset\n",
    "for train_size in np.power(2, [i for i in range(3,11)]):\n",
    "    X, ys, Thetastar = sim_ThetaXy(n_gen= train_size + test_size)\n",
    "    _, _, y_w, y_w_new = train_test_split(X, ys[0], train_size = train_size)\n",
    "    X, X_new, y_m, y_m_new = train_test_split(X, ys[1], train_size = train_size)\n",
    "    train_df, test_df = train_test_err(X, X_new, y_w, y_w_new, y_m, y_m_new, Thetastar)\n",
    "    plt.plot(train_df['err_w1'], test_df['err_new_w1'], train_df['err_w2'], test_df['err_new_w2'])\n",
    "    plt.plot(train_df['err_m1'], test_df['err_new_m1'], train_df['err_m2'], test_df['err_new_m2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZiMS-WHG-iH"
   },
   "source": [
    "# Approach 1, 2\n",
    "Main code is in Appedix. \n",
    "`argmin_lopt_gen`is plug-in optimizer from Henry's writing,`argmin_lopt_bar_argmin_lpred` is approach1,`argmin_lopt_emp` is approach2. Three optimization engine is used; \n",
    "- `optW_lopt_NV.stan` for plug-in optimizer \n",
    "- `optTheta_lpred.stan` for approach1 \n",
    "- $\\theta$ grid search within `argmin_lopt_bar_argmin_lpred` for approach2\n",
    "\n",
    "The last needs to be improved in a more continuous manner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1wZReVnI5yy"
   },
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xU4PVhEuJDlU"
   },
   "source": [
    "\n",
    "## Well-specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1630947100454,
     "user": {
      "displayName": "Hyunji Moon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiYrsHKC2ko8bXqSu_1fLdisKmzowPXNEyNDH5ejzI=s64",
      "userId": "17944317414578830628"
     },
     "user_tz": 240
    },
    "id": "FCkNpz-MtBN0",
    "outputId": "63961ee3-9323-4c9f-e7eb-899d2c69e71c"
   },
   "outputs": [],
   "source": [
    "def P_ybarx(theta, x, n_gen = 10):\n",
    "  return np.random.normal(loc = theta @ x, scale = 1, size = n_gen)\n",
    "yw = np.array(df.yw)  \n",
    "df['what1']= argmin_lopt_bar_argmin_lpred(MClass, P_ybarx, yw, X) #MClass = \"lin\"\n",
    "df['what2']= argmin_lopt_emp(P_ybarx, yw, X)\n",
    "profit = 5\n",
    "cost = 1\n",
    "print(np.mean([- (profit * min(w, y) - cost * w) for w, y in zip(df.yw, df.what1)]))\n",
    "print(np.mean([- (profit * min(w, y) - cost * w) for w, y in zip(df.yw, df.what2)]))\n",
    "# two experiments\n",
    "#-0.424, -1.706\n",
    "#-0.327, -1.522"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYL3lRuH8qwC"
   },
   "source": [
    "Approach1 is better when well-specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAQK52VwDrrf"
   },
   "source": [
    "## Misspecification type 1: degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "To7-6XsFDiT1"
   },
   "outputs": [],
   "source": [
    "ym1 = np.array(df.ym1)\n",
    "df['what1_m']= argmin_lopt_bar_argmin_lpred(MClass, P_ybarx, yw, X) #MClass = \"lin\"\n",
    "df['what2_m']= argmin_lopt_emp(P_ybarx, ym1, X)\n",
    "print(np.mean([-(profit * min(w, y) - cost * w) for w, y in zip(df.ym1, df.what1_m)]))\n",
    "print(np.mean([-(profit * min(w, y) - cost * w) for w, y in zip(df.ym1, df.what2_m)]))\n",
    "#3.23, 3.57\n",
    "#2.95, 3.08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6oZjRDBDyY1"
   },
   "source": [
    "## Misspecification type 2: distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4szd5hxFRBP"
   },
   "outputs": [],
   "source": [
    "ym2 = np.array(df.ym2)\n",
    "df['what1_m2']= argmin_lopt_bar_argmin_lpred(MClass, P_ybarx, ym2, X) #MClass = \"lin\"\n",
    "df['what2_m2']= argmin_lopt_emp(P_ybarx, ym2, X)\n",
    "print(np.mean([-(profit * min(w, y) - cost * w) for w, y in zip(df.ym2, df.what1)]))\n",
    "print(np.mean([-(profit * min(w, y) - cost * w) for w, y in zip(df.ym2, df.what2)]))\n",
    "# -1.7073\n",
    "# -1.5114\n",
    "# approach 1 better so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "executionInfo": {
     "elapsed": 460,
     "status": "ok",
     "timestamp": 1630947323558,
     "user": {
      "displayName": "Hyunji Moon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiYrsHKC2ko8bXqSu_1fLdisKmzowPXNEyNDH5ejzI=s64",
      "userId": "17944317414578830628"
     },
     "user_tz": 240
    },
    "id": "Fuyy79fBCSKJ",
    "outputId": "21b83440-d8d9-4d0b-8abb-7091643bf25c"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdJO808LEr0r"
   },
   "source": [
    "# questions\n",
    "#### Implementation\n",
    "## policy.py \n",
    "`argmin_l_pred's MClass`\n",
    "1. Can you give me some example of model class other than linear? \n",
    "2. Unlike Adam's previous comment on $y \\sim N(\\theta' X'X, 1)$ `quad` could be elementwise power $X^2 (n*p)$? (vs $X'X (p*p)$)\n",
    "3. How to improve grid search of theta in approach2?\n",
    "\n",
    "#### Experiment design\n",
    "1.  What setting could make approach 2 outperform for distribution misspecification like it does in degree of predictor misspecification? Is this misspecification type needed?\n",
    "\n",
    "#### Theory\n",
    "1. Could approach1 also be thought as empirical risk minization? As the solution depends on the dataset (x_i, y_i) at $\\theta_1$ estimation level, I think it could be. Is this is why you used $\\hat{w}$, not $w*$, Henry?\n",
    "2. Is it awkward to express the cost funciton as $l_{opt}$? Thought the input is w, y underneath, it could be implicitly rearranged as function of $\\hat{\\theta}, \\theta$.\n",
    "3. Does having a feature related to parameteric optimization?\n",
    "4. $H_\\theta := P_\\theta()$ is this notation correct?\n",
    "5. Is Remark2 (P(.) combinatorial/MIP) from SPO paper applicable for approach2? \n",
    "$w^{∗}(c) \\in \\underset{w \\in \\tilde{S}}{argmin} l_{opt}(w, y)⊆ \\underset{w \\in S}{argmin} l_{opt}(w, y)$ where S, $\\tilde{S}$ are feasible set and its closed convex hull."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0fkKInt_b7k"
   },
   "source": [
    "# Appendix\n",
    "## Main code data generator and opt_engine\n",
    "- generator\n",
    "- plugin optimizer\n",
    "- approach 1\n",
    "- approach 2\n",
    "\n",
    "much easier to view here: https://github.com/hyunjimoon/robust_optimization/tree/master/src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBNL02hc8NEe"
   },
   "source": [
    "```\n",
    "def generator(theta_star, sigma_x, sigma_y, degree, dist, n):\n",
    "    '''\n",
    "    Generate data of 1.ground truth (deg=1, norm_dist), 2.(deg>1, norm_dist), \n",
    "                     3.(deg = 1, !norm_dist), 3.(deg > 1, !norm_dist)\n",
    "\n",
    "    Parameters:\n",
    "        array theta_star: true parameter value\n",
    "        array sigma_x: array of length p, is the variance of each feature vector dimension, i.e. x_i ~ N(0, sigma_p)\n",
    "        float sigma_y: noise of outcome sampled as N(y_true, sigma_y)\n",
    "        int degree: `y_ture|x` str.  #MISSPEC1 degree 1 vs >1\n",
    "                    y_true = x-linear if degree =1, polynomial degree prop.to amount of model misspecification\n",
    "        chr dist: `y|y_ture` str. #MISSPEC2 normal_dist vs unif_dist, t_dist, gamma_dist, (TODO mm_dist)\n",
    "        int n: number of data points to generate\n",
    "        # x \\sim N(0, sigma_x)\n",
    "        # y|x \\sim N(y_true, sigma_y)\n",
    "        # (x,y) \\sim Normal(??)\n",
    "    Returns:\n",
    "        np.array X: predictor data of dimension [n, p]\n",
    "        np.array y: outcome data of dimension [n, 1] = (1,p) * (p, n)\n",
    "    '''\n",
    "    # metadata\n",
    "    p = len(theta_star)\n",
    "    # Generate predictor: iid MVN with each ith col. predictors share `sigma_x[i]`\n",
    "    X = np.random.normal(loc = 0, scale = sigma_x, size = [n, p]) # each row is a training point of size p\n",
    "    def normal_dist(mu,sd, n):\n",
    "      return np.random.normal(mu, sd, size = n)\n",
    "    def unif_dist(mu, sd, n):\n",
    "      return np.random.uniform(mu - np.sqrt(3)*sd, mu + np.sqrt(3)*sd, n)\n",
    "    if dist == 'unif':\n",
    "      ys = [P(theta_star @ np.power(X, d).T, np.repeat(sigma_y, n), n) for P in (normal_dist, unif_dist) for d in (1, degree)]\n",
    "    return X, ys\n",
    "    \n",
    "## 2. Policies with given predictors (supervised)\n",
    "# plug-in opt.solver\n",
    "def argmin_lopt_gen(P_ybarx, theta, x):\n",
    "    '''\n",
    "    Compute optimal solution for each theta and X assuming $Y|X \\sim P_{\\theta}$\n",
    "    Parameters:\n",
    "        function P_ybarx: given x, theta, simulate y vector (should be len(y)>1)\n",
    "        array theta: model parameter of size [p, 1]\n",
    "        array x: predictor data of size [1, p] \n",
    "        function lopt: loss function, given w(x), y, X, outputs loss\n",
    "    Returns:\n",
    "        array w: optimal solution for each x of size [n,1]\n",
    "    '''\n",
    "    y_sim = P_ybarx(theta, x, n_gen = 10)\n",
    "    profit, cost = 5, 1 #todo **kwargs.args()\n",
    "    data = {'y': list(y_sim), 'n': len(y_sim), 'profit': profit, 'cost': cost} \n",
    "    sm = cmdstanpy.CmdStanModel(stan_file=\"/content/drive/MyDrive/Colab Notebooks/robust_optimization/src/stan/optW_lopt_NV.stan\") \n",
    "    what = sm.optimize(data).stan_variable('w')\n",
    "    return what\n",
    "\n",
    "# approach 1\n",
    "def argmin_lopt_bar_argmin_lpred(MClass, P_ybarx, y, X, alg_type = \"sample\"):\n",
    "    '''\n",
    "    Compute approach1 optimal solution; separate predict optimize\n",
    "    Parameters:\n",
    "         char MClass: model class type specificed with `P_ybarx`\n",
    "            \"lin\": y_true = theta' * X\n",
    "            \"quad\": y_true = theta' * X^2\n",
    "          np.array y: outcome data of size [n, 1]\n",
    "          np.array X: predictor data of size [n, p] \n",
    "          chr alg_type: Solver algorithm\n",
    "    Returns:\n",
    "        np.array what: predict then optimize optimal solution of size [n,1]\n",
    "    '''\n",
    "    thetahat1 = argmin_lpred(MClass, y, X, alg_type = alg_type)\n",
    "    what = np.repeat(np.nan, len(X))\n",
    "    for i in range(len(X)):\n",
    "        what[i] = argmin_lopt_gen(P_ybarx, thetahat1, X[i])\n",
    "    return what\n",
    "\n",
    "# approach 2\n",
    "def argmin_lopt_emp(P_ybarx, y, X):\n",
    "    '''\n",
    "    Compute approach2 optimal solution; empirical objective minimization\n",
    "    Parameters:\n",
    "        function P_ybarx: simulator function given X and theta, outputs y\n",
    "        array y: outcome data of size [n, 1]\n",
    "        array X: predictor data of size [n, p] \n",
    "    Returns:\n",
    "        array what: optimal solution for each x of size [n,1]\n",
    "    '''\n",
    "    # DO NOT CONFUSE THE TWO\n",
    "    # what_x (\\hat{w}(\\theta, x_i))\n",
    "    # what (\\hat{w}(\\hat{\\theta_2}, x))\n",
    "    theta1 = np.arange(1, 3, 0.5)\n",
    "    theta2 = np.arange(1, 3, 0.5)\n",
    "    theta1, theta2 = np.meshgrid(theta1, theta2)\n",
    "    profit, cost = 5, 1\n",
    "    M = 1000000\n",
    "    for theta in zip(theta1.ravel(), theta2.ravel()):\n",
    "      loss = 0\n",
    "      for i in range(len(X)):\n",
    "        what_x = argmin_lopt_gen(P_ybarx, theta, X[i])\n",
    "        loss -= (profit * min(what_x, y[i]) - cost * what_x)\n",
    "        #lopt_NV(what, y[i], profit, cost) 'numpy.float64' object cannot be interpreted as an integer\n",
    "      if loss < M:\n",
    "        thetahat2 = theta\n",
    "    what = np.repeat(np.nan, len(X))\n",
    "    for i in range(len(X)):\n",
    "        what[i] = argmin_lopt_gen(P_ybarx, thetahat2, X[i])\n",
    "    return what\n",
    "\n",
    "def argmin_lpred(MClass, y, X, alg_type = \"sample\"):\n",
    "    '''\n",
    "    Solve parameter that best predicts y given X assuming $y\\sim N(X * theta, 1)$\n",
    "    Parameters:\n",
    "        char MClass: model class type specificed with `P_ybarx` which is user-defined \n",
    "             input for `argmin_lopt_gen`, `argmin_lopt_emp` \n",
    "             SHOULD comply with \"P_ybarx_f{MClass}.stan\"\n",
    "             \"lin\": y_true = theta' * X\n",
    "             \"quad\": y_true = theta' * X^2\n",
    "        np.array y: outcome data of size [n, 1]\n",
    "        np.array X: predictor data of size [n, p] \n",
    "        chr alg_type: Solver algorithm\n",
    "             \"sample\": MAP with improper uniform priors (undeclared) in stan file with HMC\n",
    "             \"optimize\": Maximum likelihood estimate () with \"lbfgs\", \"bfgs\", \"newton\"  \n",
    "             \"varaiational\": variational inference with ADVI, RVI\n",
    "             https://mc-stan.org/cmdstanpy/examples/Maximum%20Likelihood%20Estimation.html\n",
    "             https://mc-stan.org/cmdstanpy/examples/Variational%20Inference.html\n",
    "    Returns:\n",
    "        np.array thetahat: optimal parameter value of size [p, 1]\n",
    "    '''\n",
    "    data = {'X':X.tolist(), 'y':y, 'n': len(X), 'p': len(X[1])} \n",
    "    sm = cmdstanpy.CmdStanModel(stan_file=\"/content/drive/MyDrive/Colab Notebooks/robust_optimization/src/stan/optTheta_lpred.stan\") # true theta: 2, 2\n",
    "    if alg_type == \"sample\":\n",
    "      thetahat = np.mean(sm.sample(data).stan_variable('beta'), axis =0) # 2.00, 1.98\n",
    "    if alg_type == \"optimize\":\n",
    "      thetahat = sm.optimize(data).stan_variable('beta') # 2.00 , 1.98\n",
    "    if alg_type == \"variational\": \n",
    "      thetahat = np.mean(sm.sample(data).stan_variable('beta'), axis =0) # 1.99, 1.99\n",
    "    return thetahat\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTBlC9McawMc"
   },
   "source": [
    "EOD 0906."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM2TZV3D5nrOT6SkMCknb0D",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "SPEO.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/hyunjimoon/robust_optimization/blob/master/SPEO.ipynb",
     "timestamp": 1630795415181
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
